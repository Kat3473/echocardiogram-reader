{"cells":[{"cell_type":"markdown","metadata":{"id":"7e_rjwK0xYyh"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"Ek7ZDTT__zgv"},"source":["Importing all neccessary libraries onto the drive:\n","\n","\n","*   Standard Python Libraries\n","*   TensorFlow and Keras\n","*   Libraries for GradCAM implementation\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Firstly, the necessary libraries must imported and loaded by the Colab instance. Those of note are the NumPy, Pandas and matplotlib libraries as they will be used to manipulate the dataset and prepare it for pre-processing.\n","\n","Additionally, various parts and components of the TensorFlow and Keras libraries are imported too. Some are imported directly for convenience to avoid writing out long lines of code to access them.\n","\n","Lastly, the final snippet of code allows for the automatic mounting of the Google Drive to this Jupyter Notebook file and, therefore, access the echocardiogram dataset stored. It is worth noting that because of the way Drive works, the original user must be running the code in order for the notebook to be able to access the drive stored on their account."],"metadata":{"id":"lTHzkQ46cp6K"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"3fhF6NaUxUPO","executionInfo":{"status":"ok","timestamp":1728145153498,"user_tz":-60,"elapsed":12190,"user":{"displayName":"Katherine Htwe","userId":"16181055630253835766"}}},"outputs":[],"source":["#Import: General\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import pathlib\n","import seaborn as sn\n","import sklearn.metrics\n","\n","#Import: ResNet-50\n","from keras.preprocessing import image\n","from keras.applications import ResNet50\n","from tensorflow.keras.applications import imagenet_utils\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import tensorflow as tf\n","import keras\n","\n","#Import: Grad-CAM\n","from tensorflow.keras.models import Model\n","from google.colab.patches import cv2_imshow\n","import cv2\n","import imutils\n","\n","#Mount Drive\n","#from google.colab import drive\n","#drive.mount(\"drive\")"]},{"cell_type":"markdown","metadata":{"id":"JmN5IopcZ8XC"},"source":["# Grad-CAM"]},{"cell_type":"markdown","metadata":{"id":"wx5ioDCbZ-5p"},"source":["Grad-CAM allows to see what the CNN 'sees'. Great for analysing and understanding where the neural nodes activate and if the model works as intended."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJa0HEHcZ_S1"},"outputs":[],"source":["class GradCAM:\n","\n","  def __init__(self, model, classIdx, layerName=None):\n","    self.model = model\n","    self.classIdx = classIdx\n","    self.layerName = layerName\n","\n","    if self.layerName is None:\n","      self.layerName = self.find_target_layer()\n","\n","  def find_target_layer(self):\n","    for layer in reversed(self.model.layers): #loops through all layers from the last\n","      if len(layer.output_shape) == 4: #until it finds a layer with an output shape of 4 as required\n","        print(layer) #otherwise the Grad-CAM will not work as a heatmap cannot be built\n","        return layer.name\n","    raise ValueError(\"Could not find a 4D layer. GradCAM cannot work.\") #for incompatiable models\n","\n","  def compute_heatmap(self, image, eps=1e-8): #constructs the heatmap\n","    gradModel = Model ( #creating a blank model object and assinging the CNN model to it\n","\n","\n","        inputs = [self.model.inputs],\n","        outputs = [self.model.get_layer(self.layerName).output, self.model.output]\n","    )\n","\n","    with tf.GradientTape() as Tape: #creates the gradient tape object\n","      inputs = tf.cast(image, tf.float32)\n","      (convOutputs, predictions) = gradModel(inputs) #runs the prediction and grabs the results\n","      loss = predictions[:, self.classIdx] #grabs the loss value from the results\n","      grads = Tape.gradient(loss, convOutputs) #finds the gradient values\n","\n","    castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n","    castGrads = tf.cast(grads > 0, \"float32\")\n","    guidedGrads = castConvOutputs * castGrads * grads #calculates the guided gradient values\n","\n","    convOutputs = convOutputs[0]\n","    guidedGrads = guidedGrads[0]\n","\n","    weights = tf.reduce_mean(guidedGrads, axis=(0, 1)) #calculates weight values\n","    cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1) #finds the final values to be used by the heatmap\n","\n","    (w, h) = (image.shape[2], image.shape[1]) #begins drawing out the heatmap\n","    heatmap = cv2.resize(cam.numpy(), (w, h)) #resizing to fit the input image dimensions\n","    numer = heatmap - np.min(heatmap)\n","    denom = (heatmap.max() - heatmap.min()) + eps\n","    heatmap = numer / denom\n","    heatmap = (heatmap * 255).astype(\"uint8\")\n","\n","    return heatmap #returns the heatmap as an output of this function\n","\n","  def overlay_heatmap(self, heatmap, image, alpha=0.5, colourmap=cv2.COLORMAP_VIRIDIS): #Overlays the heatmap onto the image\n","    heatmap = cv2.applyColorMap(heatmap, colourmap)\n","    output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n","\n","    return heatmap, output #returns both the heatmap and the final output image"]},{"cell_type":"markdown","metadata":{"id":"x72VgjJfxfn4"},"source":["# Building the Database"]},{"cell_type":"markdown","metadata":{"id":"WMSRQwsz_xc0"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ex9RLXg5xfRr"},"outputs":[],"source":["#Defining dataset parameters\n","batch_size = 32 #Size of batches for the data\n","img_height = 224 #Height of the images\n","img_width = 224 #Width of the images\n","\n","#Grabbing the echocardiogram folder\n","dataset_url = \"/content/drive/MyDrive/Project/dataset/echocardiograms\" #google drive of the database\n","data_dir = pathlib.Path(dataset_url).with_suffix('') #sets up the directory to be used\n","classnames = ['a4c', 'a2c'] #Array to assign labels to the classes [1, 0]\n","\n","#Training dataset\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir, #the directory being converted\n","  validation_split=0.2, #20% used for validation, 80% for actual training\n","  subset=\"training\", #this is the training subset being outputted\n","  seed=132, #seed to randomise data order and seperation\n","  image_size=(img_height, img_width), #converts image sizes to required\n","  batch_size=batch_size, #the batch size of the dataset\n","  class_names=classnames, #assinging labels to the classes\n","  label_mode='int' #controls the datatype of the class labels i.e. [1, 0]\n","  )\n","\n","#Validation dataset\n","val_ds = tf.keras.utils.image_dataset_from_directory( #same as above but for the validation dataset\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\", #for validation subset\n","  seed=132,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  class_names=classnames,\n","  label_mode='int'\n","  )\n","\n","#Manually records the class labels for each echocardiogram in the validation dataset for later use\n","labels =  np.array([])\n","labels = np.concatenate([y for x, y in val_ds], axis=0)\n","\n","#Dataset pre-processing\n","preprocess_input = keras.applications.resnet.preprocess_input #data pre-processing function for ResNet-50\n","\n","#Running the echocardiogram images through a pre-processing neural network layer and saving the new results\n","train_ds = train_ds.map(lambda images, labels:\n","                        (preprocess_input(images), labels))\n","val_ds = val_ds.map(lambda images, labels:\n","                    (preprocess_input(images), labels))"]},{"cell_type":"markdown","metadata":{"id":"vrOU3ZkbBz5m"},"source":["# ResNet-50"]},{"cell_type":"markdown","metadata":{"id":"kCEcXrjuB4sX"},"source":["Instantiating a ResNet50 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PRzQbgoB5Ko"},"outputs":[],"source":["#Pre-trained ResNet-50 v2 model\n","resnet50 = ResNet50(\n","    include_top=False, #Does not include the top layers used for classification\n","    weights='imagenet', #pre-trained using the imagenet dataset\n","    input_shape=(img_height,img_width,3), #size of the input images\n","    pooling=None, #no additional pooling layers\n","    classes=1000 #irrelevant here as no top layers included\n",")"]},{"cell_type":"markdown","metadata":{"id":"RGRJAQqTsOkd"},"source":["Applying transfer learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ec_hh_BvsTD8"},"outputs":[],"source":["#Freezing the resnet model\n","resnet50.trainable = False #sets resnet50 to be untrainable\n","\n","#Sequentional New Model\n","model = keras.Sequential() #keras structure to create a CNN model by sequentially adding layers\n","model.add(resnet50) #adds resnet50 as the first 'layer'\n","model.add(keras.layers.Identity()) #blank layer used by Grad-CAM to peer into the model. The layer does nothing.\n","model.add(keras.layers.GlobalAveragePooling2D()) #Pooling layer that takes a global average of its input and outputs the answer.\n","model.add(keras.layers.Dropout(0.2)) #Dropout layer is used to reduce overfitting in models by 'dropping out' by setting some of the input units to 0 at a rate of its function input (0.2).\n","model.add(keras.layers.Dense(2, activation='softmax')) #A fully-connected (Dense) layer with a softmax activiation function to output an array of probabilities for the viewpoints.\n","\n","#Model Summary\n","model.summary(show_trainable=True) #summary of the final model and its layers"]},{"cell_type":"markdown","metadata":{"id":"s0BWFfgEII3N"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"jychXfjQIMNE"},"source":["Training and validating the CNN model. As only the top layers are set to be trainable, only they will be trained. However, the entire model is validated.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WB7uVdMrIMiu"},"outputs":[],"source":["#Setup\n","callback_list = [keras.callbacks.CSVLogger('/content/drive/MyDrive/Project/results/epoch_log.csv', separator=\",\", append=False), #will log results per epoch into a csv file\n","                 keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Project/results/tensor_logs')] #will take logs of the tensors for debugging\n","\n","\n","#Compiling the model\n","model.compile(\n","  optimizer='adamax', #using adamax optimizer\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(), #using sparce categorical crossentropy loss function\n","  metrics=[keras.metrics.SparseCategoricalAccuracy(), #using accuracy (multi-class model version) and poisson metrics\n","           keras.metrics.Poisson()]\n",")\n","\n","#Training the model\n","score = model.fit(\n","  train_ds, #training dataset\n","  validation_data=val_ds, #validation dataset\n","  epochs=20, #number of training steps i.e. number of times the model is trained using the training dataset\n","  callbacks=callback_list, #runs additional functions in between training\n","  use_multiprocessing=True #Uses multi-processing for better computational power usage\n",").history #Recording the metrics into the array 'score'"]},{"cell_type":"markdown","metadata":{"id":"17y5Cw0sQ6CI"},"source":["# Model Cache"]},{"cell_type":"markdown","metadata":{"id":"jc5dUgGYBwkg"},"source":["This allows for the saving and loading (i.e. storage) of the CNN model. As a result, various versions of the model can be trained, saved and then loaded as needed.\n","\n","Model versions (and file name):\n","\n","*   Final CNN model (Model)\n","*   CNN model with 256x256 resolution (Model_256)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uj2OulAC-2a-"},"outputs":[],"source":["#Saving the Model\n","model.save(\"/content/drive/MyDrive/Project/code/Model\")\n","model.summary(show_trainable=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nP5aZCilbvUt"},"outputs":[],"source":["#Load Saved Model\n","model = keras.models.load_model(\"/content/drive/MyDrive/Project/code/Model\")\n","model.summary(show_trainable=True)"]},{"cell_type":"markdown","metadata":{"id":"12qFqMAeB1f4"},"source":[]},{"cell_type":"markdown","metadata":{"id":"dmaxgrwzKjCe"},"source":["# Results"]},{"cell_type":"markdown","metadata":{"id":"axrJlGaSKmdk"},"source":["Uses the metrics obtained from the training section to plot out graphs for them against each training step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAY_wiYLKm0B"},"outputs":[],"source":["#Plotting Loss per epoch\n","plt.figure()\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Training Steps\")\n","plt.ylim([0,1])\n","line1 = plt.plot(score[\"loss\"], label='Training')\n","line2 = plt.plot(score[\"val_loss\"], label='Validation')\n","plt.legend()\n","\n","#Plotting Accuracy per epoch\n","plt.figure()\n","plt.ylabel(\"Accuracy\")\n","plt.xlabel(\"Training Steps\")\n","plt.ylim([0,1])\n","plt.plot(score[\"sparse_categorical_accuracy\"], label='Training')\n","plt.plot(score[\"val_sparse_categorical_accuracy\"], label='Validation')\n","plt.legend()\n","\n","#Plotting Poisson per epoch\n","plt.figure()\n","plt.ylabel(\"Poisson\")\n","plt.xlabel(\"Training Steps\")\n","plt.ylim([0,1])\n","plt.plot(score[\"poisson\"], label='Training')\n","plt.plot(score[\"val_poisson\"], label='Validation')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"yh7wdlsprdGn"},"source":["# Testing"]},{"cell_type":"markdown","metadata":{"id":"ASUXjXTyrgkl"},"source":["This section is used for manually testing and debugging the model. Images from a testset were used to analyse the inner-workings of the CNN model to better understand it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP36DvZ_3ItC"},"outputs":[],"source":["#Preparing the test image\n","url = \"/content/drive/MyDrive/Project/dataset/testset/ES00058 N_4CH_2_002.jpg\" #Taking a test image by its url\n","orig = cv2.imread(url)\n","resized = cv2.resize(orig, (img_width, img_height)) #resizing it\n","\n","image = load_img(url, target_size=(img_width,img_height)) #loading the image\n","image = img_to_array(image) #converting image into an array for pre-processing and classification\n","image = np.expand_dims(image, axis=0)\n","image = train_ds = keras.applications.resnet.preprocess_input(image) #pre-processing the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mtCwFBWrfu_"},"outputs":[],"source":["#Prediction\n","preds = model.predict(image) #classifying the image\n","i = np.argmax(preds[0]) #Grabbing the classification from the output array\n","label = classnames[np.argmax(preds[0])] + \": \" + str(max(preds[0])) #Translating it to be human-readable\n","\n","print(preds[0]) #Prints array of predictions\n","print(\"The predicted class is\", classnames[np.argmax(preds[0])]) #Prints the human-readable class label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uj-cKhcNARS_"},"outputs":[],"source":["#Grad CAM\n","cam = GradCAM(model, i) #Created grad-cam object\n","heatmap = cam.compute_heatmap(image) #calculates the heatmap for the test image\n","heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0])) #resizes the heatmap to match image dimensions\n","(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5) #overlays the heatmap onto the image\n","\n","cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1) #adding a label indicating the softmax prediction probability value\n","cv2.putText(output, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n","\n","output = np.vstack([orig, heatmap, output]) #creating a stack of images\n","output = imutils.resize(output, height=700) #resizing the stack\n","cv2_imshow(output) #displaying the results\n","cv2.waitKey(0)"]},{"cell_type":"markdown","source":["## Confusion Matrix"],"metadata":{"id":"U52iEmD5pI5m"}},{"cell_type":"code","source":["y_prediction = model.predict(val_ds) #manually performing a validation check again\n","y_pred = np.argmax(y_prediction, axis=1) #grabbing the prediction values"],"metadata":{"id":"xf6RDLnZk6CH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = tf.math.confusion_matrix(labels, y_pred, num_classes=2) #calculates the confusion matrix\n","df_cm = pd.DataFrame(result, range(2), range(2)) #converts it into the Pandas Dataframe object\n","sn.set(font_scale=1.4) #Sets label size\n","hm = sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) #Creates a heatmap-styled figure\n","plt.show() #displays result"],"metadata":{"id":"LUGgN2NpqGjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#calculates precision and recall for double-checking the manual calculations\n","matrix = sklearn.metrics.classification_report(labels, y_pred, target_names=classnames, output_dict=False)\n","print(matrix)"],"metadata":{"id":"AymUWlJWjCxJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"476LGfpkIJ0f"},"source":["# Model Infomation"]},{"cell_type":"markdown","metadata":{"id":"ZO_th2dYINwk"},"source":["This code can be used to create a block diagram of the final CNN model and the ResNet-50 model used. The block diagrams show every layer as well as relvant information for each layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJ0EIItnINK0"},"outputs":[],"source":["#Model Information\n","keras.utils.plot_model(model, to_file='/content/drive/MyDrive/Project/results/model.png', show_shapes=True)\n","keras.utils.plot_model(resnet50, to_file='/content/drive/MyDrive/Project/results/resnet50.png', show_shapes=True)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["17y5Cw0sQ6CI"],"gpuType":"T4","provenance":[],"mount_file_id":"10KhXzD60nI6YrJZ4FOnWUI03a3zKYz47","authorship_tag":"ABX9TyOZmZ2buTkDv/dlr8ViNeBK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}